{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"최종_ImageAugmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqQmFnROUgFJ","outputId":"2af1f9e4-9cec-4368-8321-5b3665d598dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Cnn_classification/cnn_img/')"],"metadata":{"id":"VGMiYfwZUjlY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##비디오 프레임별로 저장"],"metadata":{"id":"oYQ7WpwBPrZf"}},{"cell_type":"code","source":["import cv2\n","vidcap = cv2.VideoCapture('/content/drive/MyDrive/video/KakaoTalk_20220331_201401471.mp4') # 비디오 파일 경로\n","success,image = vidcap.read()\n","\n","count = 0\n","success = True\n","\n","while success:\n","  success,image = vidcap.read()\n","  cv2.imwrite(\"/content/drive/MyDrive/video/img/%d.jpg\" % count, image) # 저장할 폴더와 이름 지정\n","  print(\"saved image %d.jpg\" % count)\n","  \n","  if cv2.waitKey(10) == 27:                    \n","      break\n","  count += 1"],"metadata":{"id":"z7U5FLZaPq5X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Face recognition"],"metadata":{"id":"0j9l_tX4Uu6g"}},{"cell_type":"code","source":["!pip install face_recognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DeUveFfUxs_","outputId":"321517ff-d7f2-4d4f-c3e5-380d9ceb630e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n","Collecting face-recognition-models>=0.3.0\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[K     |████████████████████████████████| 100.1 MB 23 kB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.21.5)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=85984f2370295b918e58983e6f6e86d4e3b758019ef84b403a51f57a74a270ad\n","  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face-recognition\n","Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import face_recognition as frc\n","import time\n","import cv2\n","\n","groups_folder_path = 'original/'\n","start = time.time()\n","\n","image_w = 224\n","image_h = 224\n","categories = [\"경한\", \"민용\", \"연수\", \"원\", \"유리\", \"혜령\"]\n","\n","# 얼굴 인식 후 인식된 얼굴 저장\n","for idex, categorie in enumerate(categories):\n","    image_dir = groups_folder_path + categorie + '/'\n","    \n","    # image_dir 경로의 모든 파일들에 대하여 작업\n","    for top, dir, f in os.walk(image_dir):\n","        # 경로내 모든 파일에 대하여\n","        for filename in f:\n","            print(image_dir+filename)\n","            \n","            # 파일불러오기\n","            img = frc.load_image_file(image_dir+filename)\n","            \n","            # 파일 얼굴인식\n","            faces = frc.face_locations(img)\n","            \n","            # 얼굴 인식\n","            if len(faces) == 1:\n","                for face_location in faces:\n","                    top, right, bottom, left = face_location\n","                    face_image = img[top:bottom, left:right]\n","                    resize_img = cv2.resize(face_image, dsize = (224,224))\n","                    rgb_img = cv2.cvtColor(resize_img, cv2.COLOR_BGR2RGB)\n","                    cv2.imwrite('face_only/'+ categorie + '/' + filename, rgb_img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlqEn4ZtUy22","outputId":"49964304-4594-4cf7-85ae-f23c7c5bbf3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["original/경한/KakaoTalk_20220405_235828458.png\n","original/경한/KakaoTalk_20220405_235828583.png\n","original/경한/KakaoTalk_20220405_235828691.png\n","original/경한/KakaoTalk_20220405_235829031.png\n","original/경한/KakaoTalk_20220405_235829224.png\n","original/경한/KakaoTalk_20220405_235829389.png\n","original/경한/KakaoTalk_20220405_235829639.png\n","original/경한/KakaoTalk_20220405_235829888.png\n","original/경한/KakaoTalk_20220406_000604303.png\n","original/경한/7.JPG\n","original/민용/KakaoTalk_20220405_235811769_02.jpg\n","original/민용/KakaoTalk_20220405_235811769_06.jpg\n","original/민용/KakaoTalk_20220405_235811769_05.jpg\n","original/민용/KakaoTalk_20220405_235811769_07.jpg\n","original/민용/KakaoTalk_20220405_235811769_04.jpg\n","original/민용/KakaoTalk_20220405_235824294.jpg\n","original/민용/1.JPG\n","original/민용/2.JPG\n","original/민용/3.JPG\n","original/민용/4.JPG\n","original/연수/KakaoTalk_20220405_235026140.jpg\n","original/연수/KakaoTalk_20220405_235027318.jpg\n","original/연수/KakaoTalk_20220405_235028478.jpg\n","original/연수/KakaoTalk_20220405_235029303.jpg\n","original/연수/KakaoTalk_20220405_235029944.jpg\n","original/연수/KakaoTalk_20220405_235030904.jpg\n","original/연수/KakaoTalk_20220405_235031533.jpg\n","original/연수/KakaoTalk_20220405_235026613.jpg\n","original/연수/66.JPG\n","original/연수/5.JPG\n","original/원/KakaoTalk_20220406_003054429_08.jpg\n","original/원/KakaoTalk_20220406_003054429_05.jpg\n","original/원/KakaoTalk_20220406_003054429_01.jpg\n","original/원/KakaoTalk_20220406_003054429.jpg\n","original/원/KakaoTalk_20220406_003054429_07.jpg\n","original/원/KakaoTalk_20220406_003054429_02.jpg\n","original/원/KakaoTalk_20220406_003054429_09.jpg\n","original/원/KakaoTalk_20220406_003054429_06.jpg\n","original/원/KakaoTalk_20220406_003054429_04.jpg\n","original/원/KakaoTalk_20220406_003054429_03.jpg\n","original/유리/KakaoTalk_20220405_235215164.jpg\n","original/유리/KakaoTalk_20220405_235215164_02.jpg\n","original/유리/KakaoTalk_20220405_235215164_01.jpg\n","original/유리/KakaoTalk_20220405_235215164_03.jpg\n","original/유리/KakaoTalk_20220405_235215164_04.jpg\n","original/유리/KakaoTalk_20220405_235215164_07.jpg\n","original/유리/KakaoTalk_20220405_235215164_05.jpg\n","original/유리/KakaoTalk_20220405_235215164_06.jpg\n","original/유리/KakaoTalk_20220405_235215164_08.jpg\n","original/유리/KakaoTalk_20220405_235215164_09.jpg\n","original/유리/KakaoTalk_20220406_000751169.jpg\n","original/혜령/KakaoTalk_20220330_011654059_06.jpg\n","original/혜령/hyeryung_2.jpg\n","original/혜령/hyeryung_5.jpg\n","original/혜령/hyeryung_7.jpg\n","original/혜령/hyeryung_3.jpg\n","original/혜령/hyeryung_0.jpg\n","original/혜령/hyeryung_1.jpg\n","original/혜령/hyeryung_8.jpg\n","original/혜령/hyeryung_12.jpg\n","original/혜령/hyeryung_14.jpg\n","original/혜령/8.JPG\n"]}]},{"cell_type":"markdown","source":["##Augmentation"],"metadata":{"id":"c5BZZ_UMV8xi"}},{"cell_type":"code","source":["import face_recognition as frc\n","from keras.preprocessing.image import ImageDataGenerator\n","import os\n","Img_create = ImageDataGenerator(rotation_range = 30,  # 30도 이내의 회전\n","                                horizontal_flip=True, # 좌우반전\n","                                brightness_range = [0.6,1.0],  # 밝기조절\n","                                fill_mode='nearest')\n","face_dir = 'face_only/원/'  # Augmentation 적용할 이미지가 있는 폴더\n","i = 0\n","# image_dir 경로의 모든 파일들에 대하여 작업\n","for top, dir, f in os.walk(face_dir):\n","    # 경로내 모든 파일에 대하여\n","    for filename in f:\n","        print(face_dir+filename)\n","            \n","        # 파일불러오기\n","        face_img = frc.load_image_file(face_dir+filename)\n","        face_img = face_img.reshape((1,) + face_img.shape)\n","        \n","        for batch in Img_create.flow(face_img, batch_size = 1, \n","                                     save_to_dir = face_dir, save_format = 'jpg', save_prefix = 'new_' + filename):\n","            i += 1\n","            if i % 5 == 0:\n","                i = 0\n","                break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inOsn46LV-1p","outputId":"d4097bfb-fae9-437c-9e27-1839f922cf4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["face_only/원/KakaoTalk_20220406_003054429_08.jpg\n","face_only/원/KakaoTalk_20220406_003054429_05.jpg\n","face_only/원/KakaoTalk_20220406_003054429_01.jpg\n","face_only/원/KakaoTalk_20220406_003054429.jpg\n","face_only/원/KakaoTalk_20220406_003054429_07.jpg\n","face_only/원/KakaoTalk_20220406_003054429_02.jpg\n","face_only/원/KakaoTalk_20220406_003054429_09.jpg\n","face_only/원/KakaoTalk_20220406_003054429_06.jpg\n","face_only/원/KakaoTalk_20220406_003054429_04.jpg\n"]}]},{"cell_type":"markdown","source":["## (+) Image crop"],"metadata":{"id":"BIWxXjSTM6Gg"}},{"cell_type":"code","source":["import cv2\n","from PIL import Image \n","import glob\n","import pandas as pd\n","\n","count = 1\n","path = 'face_only/경한' # crop 적용할 이미지가 있는 폴더\n","filepath = glob.glob(path + '/*')\n","\n","for i in range(len(filepath)):\n","    image1 = Image.open(filepath[i])\n","    croppedImage=image1.crop((10,10,180,180))\n","    croppedImage.save('crop/경한/croppedImage_%d.jpg'%count)\n","    count += 1"],"metadata":{"id":"wHITWJjCJD--"},"execution_count":null,"outputs":[]}]}